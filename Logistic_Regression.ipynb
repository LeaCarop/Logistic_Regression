{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Regresión Logísitica y algoritmo de Gradiente Descendente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X = data['data']\n",
    "\n",
    "Y = 1 - data['target']\n",
    "print (data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x)) \n",
    "    return s\n",
    "    \n",
    "\n",
    "def gradient(X, Y, beta):\n",
    "    g = np.dot(X, beta.T) * Y\n",
    "    ds = -Y * (1 - sigmoid(g)) * X\n",
    "    return ds.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Defino la clase 'LogisticRegression'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learn_rate, intercept = True):\n",
    "        self.beta = None\n",
    "        self.learning_rate = learn_rate\n",
    "        \n",
    "              \n",
    "    def train(self, input_X, input_Y, n_epochs):    \n",
    "        intercept = np.zeros((input_X.shape[0], 1))\n",
    "        X = np.concatenate((intercept, input_X), axis = 1)\n",
    "        Y = (2 * input_Y - 1).reshape(-1, 1)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        m = X.shape[1]\n",
    "        self.beta = np.zeros([1, m])   \n",
    "        \n",
    "        for n in range(n_epochs):\n",
    "            self.beta = self.beta - self.learning_rate * gradient(X, Y, self.beta)\n",
    "        return self.beta\n",
    "    \n",
    "    def query(self, X, prob=True, cutoff=0.5):  #equivalente a PREDICT\n",
    "            intercept = np.zeros((np.shape(X)[0],1))\n",
    "            X = np.concatenate((intercept, X), 1)\n",
    "            linear_model = np.dot(X, self.beta.T)\n",
    "            y_hat = sigmoid(linear_model)      \n",
    "                \n",
    "            if prob==True:\n",
    "                return y_hat\n",
    "                \n",
    "            else: \n",
    "                return (y_hat > cutoff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-8ab31abe148e>:2: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y predecir.\n",
    "LR = LogisticRegression(learning_rate)\n",
    "LR.train(X_train, Y_train, 2000)\n",
    "Y_pred = LR.query(X_test,prob=False,cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.912\n"
     ]
    }
   ],
   "source": [
    "# Mostrar la exactitud\n",
    "acc = (Y_pred == Y_test.reshape(-1,1)).mean()\n",
    "print('Accuracy : {}'.format(np.round(acc,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy mide el porcentaje de casos que este modelo acierta, en este caso, acierta el 91,2% de las veces. No obstante, hay que tener cuidado con esta métrica, ya que si se trata de clases desbalanceadas puede dar por bueno un modelo que no lo es."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
